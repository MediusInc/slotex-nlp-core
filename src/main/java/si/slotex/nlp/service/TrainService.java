package si.slotex.nlp.service;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.List;
import java.util.stream.Collectors;

import org.apache.commons.io.FileUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import si.slotex.nlp.exception.TrainLanguageNotSupportedException;
import si.slotex.nlp.entity.CorpusSentenceDiff;
import si.slotex.nlp.entity.DocTrain;
import si.slotex.nlp.entity.ModelTrainInfo;
import si.slotex.nlp.entity.Sentence;
import si.slotex.nlp.entity.Token;
import si.slotex.nlp.opennlp.NERTask;
import si.slotex.nlp.opennlp.Task;
import si.slotex.nlp.opennlp.TaskFactory;
import si.slotex.nlp.repository.CorpusSentenceDiffRepository;
import si.slotex.nlp.repository.ModelTrainInfoRepository;
import si.slotex.nlp.utils.Constants;
import si.slotex.nlp.utils.NLPUtils;

/**
 * Used for training models with new provided data. The data has to be tagged by humans.
 *
 * @author Mitja Kotnik
 * @version 1.0
 */
@Service
public class TrainService
{
    private static Logger logger = LoggerFactory.getLogger(TrainService.class);

    @Autowired
    private ModelTrainInfoRepository modelTrainRepository;

    @Autowired
    private CorpusSentenceDiffRepository sentenceDiffRepository;

    private ModelTrainInfo modelTrainInfo;

    @Value("${file.uploadDir}")
    private String uploadDir;

    @Value("${file.data}")
    private String dataDir;

    @Value("${file.data.dictionary}")
    private String dictionaryDir;

    @Value("${file.data.train.ner}")
    private String nerTrainDir;

    @Value("${file.models}")
    private String modelFilePath;

    @Value("file.models.backup")
    private String modelBckpFilePath;

    /**
     * Is the entrypoint method for training new models for the NLP workflow.
     * Based on the modelsToTrain value it executes the following trainings.
     *
     * @param docTrain contains additional training info for new model training
     * @return info about the training that was done.
     */
    public ModelTrainInfo trainDocument(DocTrain docTrain)
    {
        if (docTrain.getLanguage().equals("eng"))
        {
            throw new TrainLanguageNotSupportedException("Training for english language is not yet supported!");
        }
        modelTrainInfo = new ModelTrainInfo();
        String modelsToTrain = docTrain.getModelsToTrain();
        if (modelsToTrain.contains("lang"))
        {
            logger.info("LANGUAGE model workflow starting...");
            trainDocumentLang(docTrain);
            logger.info("LANGUAGE model creation finnished!");
        }
        else if (modelsToTrain.contains("sentence"))
        {
            logger.info("SENTENCE model workflow starting...");
            trainDocumentSentence(docTrain);
            logger.info("SENTENCE model creation finnished!");
        }
        else if (modelsToTrain.contains("tokenizer"))
        {
            logger.info("TOKENIZER model workflow starting...");
            trainDocumentToken(docTrain);
            logger.info("TOKENIZER model creation finnished!");
        }
        else if (modelsToTrain.contains("pos"))
        {
            logger.info("PART-of-SPEECH model workflow starting...");
            trainDocumentPOS(docTrain);
            logger.info("PART-of-SPEECH model creation finnished!");
        }
        else if (modelsToTrain.contains("lemma"))
        {
            logger.info("LEMMATIZATION model workflow starting...");
            trainDocumentLemma(docTrain);
            logger.info("LEMMATIZATION model creation finnished!");
        }
        else if (modelsToTrain.contains("ner"))
        {
            logger.info("NAMED-ENTITY-RECOGNITION model workflow starting...");
            trainDocumentNER(docTrain);
            logger.info("NAMED-ENTITY-RECOGNITION model creation finnished!");
        }

        modelTrainInfo.setTimeStamp(Calendar.getInstance().getTime());
        List<String> trainSentences = new ArrayList<>();
        for (Sentence sentence : docTrain.getTrainSentences())
        {
            trainSentences.add(sentence.getSentence());
        }

        modelTrainInfo.setAdditionalTrainData(trainSentences);
        return modelTrainInfo;
    }

    /**
     * It initializes a {@code task} that is generated by our {@code TaskFactory}. The model initialization
     * is based on the {@code doTask} parameter.
     *
     * @param docTrain contains additional data for traning
     * @param doTask it specifies which model is going to be used
     * @param modelFile path to the model which is going to be retrained
     * @param trainDataFile training data file which contains training data
     */
    private void trainDocumentTask(DocTrain docTrain, String doTask, String modelFile, String trainDataFile)
    {
        Task task = TaskFactory.createTask(doTask);
        modelTrainInfo.setModelName((modelTrainInfo.getModelName() != null ? modelTrainInfo.getModelName() + ", " + modelFile : modelFile));

        try
        {
            logger.info("Starting to train model...");
            task.train(modelFile, trainDataFile, docTrain.getLanguage());
        }
        catch (IOException e)
        {
            logger.error("There was an error when building new training model.");
            logger.error("Error: " + e.toString());
            return;
        }

        logger.info("Successfully trained new model with data from: " + trainDataFile);
    }

    /**
     * Method for training a new sentence model based on the {@code docTrain}.
     *
     * @param docTrain file containing all the training data for the new model
     */
    private void trainDocumentSentence(DocTrain docTrain)
    {
        Long timeInMills = Calendar.getInstance().getTimeInMillis();
        List<String> lines = new ArrayList<>();
        for (Sentence sentence : docTrain.getTrainSentences())
        {
            lines.add(sentence.getSentence());
        }

        String trainDataFile = NLPUtils.saveTrainFile(lines, "stc", timeInMills);

        NLPUtils.saveModelBackup("/" + docTrain.getLanguage() + Constants.sentenceModelFile, modelFilePath, modelBckpFilePath, timeInMills);

        trainDocumentTask(docTrain, "stc", modelFilePath + "/" + docTrain.getLanguage() + Constants.sentenceModelFile, trainDataFile);

        saveModelTrainingInfo("/" + docTrain.getLanguage() + Constants.sentenceModelFile, trainDataFile, timeInMills, lines);
    }

    /**
     * Method for training a new language model based on the {@code docTrain}.
     *
     * @param docTrain file containing all the training data for the new model
     */
    private void trainDocumentLang(DocTrain docTrain)
    {
        Long timeInMills = Calendar.getInstance().getTimeInMillis();
        List<String> lines = new ArrayList<>();
        for (Sentence sentence : docTrain.getTrainSentences())
        {
            lines.add(docTrain.getLanguage() + "\t" + sentence.getSentence());
        }

        String trainDataFile = NLPUtils.saveTrainFile(lines, "lang", timeInMills);

        NLPUtils.saveModelBackup(Constants.langModelFile, modelFilePath, modelBckpFilePath, timeInMills);

        trainDocumentTask(docTrain, "lang", modelFilePath + Constants.langModelFile, trainDataFile);

        saveModelTrainingInfo(Constants.langModelFile, trainDataFile, timeInMills, lines);
    }

    /**
     * Method for training a new tokenizer model based on the {@code docTrain}.
     *
     * @param docTrain file containing all the training data for the new model
     */
    private void trainDocumentToken(DocTrain docTrain)
    {
        Long timeInMills = Calendar.getInstance().getTimeInMillis();
        List<String> lines = new ArrayList<>();
        for (Sentence sentence : docTrain.getTrainSentences())
        {
            lines.add(sentence.getSentence()
                    .replaceAll("\\.$", "<SPLIT>.")
                    .replaceAll(",", "<SPLIT>,")
                    .replaceAll("\\?", "<SPLIT>?")
                    .replaceAll("!", "<SPLIT>!"));
        }

        String trainDataFile = NLPUtils.saveTrainFile(lines, "token", timeInMills);

        NLPUtils.saveModelBackup("/" + docTrain.getLanguage() + Constants.tokenizeModelFile, modelFilePath, modelBckpFilePath, timeInMills);

        trainDocumentTask(docTrain, "token", modelFilePath + "/" + docTrain.getLanguage() + Constants.tokenizeModelFile, trainDataFile);

        saveModelTrainingInfo("/" + docTrain.getLanguage() + Constants.tokenizeModelFile, trainDataFile, timeInMills, lines);
    }

    /**
     * Method for training a new part-of-speech tagger model based on the {@code docTrain}.
     *
     * @param docTrain file containing all the training data for the new model
     */
    private void trainDocumentPOS(DocTrain docTrain)
    {
        Long timeInMills = Calendar.getInstance().getTimeInMillis();
        List<String> lines = new ArrayList<>();
        for (Sentence sentence : docTrain.getTrainSentences())
        {
            StringBuilder sb = new StringBuilder();
            for (Token token : sentence.getTokens())
            {
                sb.append(token.getWord() + "_" + token.getPosTag() + " ");
            }
            sb.deleteCharAt(sb.lastIndexOf(" "));
            lines.add(sb.toString());
        }

        String trainDataFile = NLPUtils.saveTrainFile(lines, "pos", timeInMills);

        NLPUtils.saveModelBackup("/" + docTrain.getLanguage() + Constants.posModelFile, modelFilePath, modelBckpFilePath, timeInMills);

        trainDocumentTask(docTrain, "pos", modelFilePath + "/" + docTrain.getLanguage() + Constants.posModelFile, trainDataFile);

        saveModelTrainingInfo("/" + docTrain.getLanguage() + Constants.posModelFile, trainDataFile, timeInMills, lines);
    }

    /**
     * Method for training a new lemmalization model based on the {@code docTrain}.
     *
     * @param docTrain file containing all the training data for the new model
     */
    private void trainDocumentLemma(DocTrain docTrain)
    {
        Long timeInMills = Calendar.getInstance().getTimeInMillis();
        List<String> lines = new ArrayList<>();
        for (Sentence sentence : docTrain.getTrainSentences())
        {
            for (Token token : sentence.getTokens())
            {
                lines.add(token.getWord() + "\t" + token.getPosTag() + "\t" + token.getLemma());
            }
            lines.add("");
        }

        String trainDataFile = NLPUtils.saveTrainFile(lines, "lemma", timeInMills);

        NLPUtils.saveModelBackup("/" + docTrain.getLanguage() + Constants.lemmaModelFile, modelFilePath, modelBckpFilePath, timeInMills);

        trainDocumentTask(docTrain, "lemma", modelFilePath + "/" + docTrain.getLanguage() + Constants.lemmaModelFile, trainDataFile);

        saveModelTrainingInfo("/" + docTrain.getLanguage() + Constants.lemmaModelFile, trainDataFile, timeInMills, lines);
    }

    /**
     * Method for training a new named-entity-recognition model based on the {@code docTrain}.
     *
     * @param docTrain file containing all the training data for the new model
     */
    private void trainDocumentNER(DocTrain docTrain)
    {
        String trainNerEntities = docTrain.getModelsToTrain();
        String entityToTrain = trainNerEntities.substring(trainNerEntities.indexOf("[") + 1);
        entityToTrain = entityToTrain.substring(0, entityToTrain.indexOf("]"));

        Long timeInMills = Calendar.getInstance().getTimeInMillis();
        List<String> lines = new ArrayList<>();
        for (Sentence sentence : docTrain.getTrainSentences())
        {
            StringBuilder sb = new StringBuilder();
            for (Token token : sentence.getTokens())
            {
                if (token.getNerTag() != null)
                {
                    sb.append("<START:" + entityToTrain + "> " + token.getWord() + " <END> ");
                }
                else
                {
                    sb.append(token.getWord() + " ");
                }
            }
            sb.deleteCharAt(sb.lastIndexOf(" "));
            lines.add(sb.toString());
        }

        String trainDataFile = NLPUtils.saveTrainFile(lines, "ner-" + entityToTrain, timeInMills);

        String modelName = getCorrectModel(entityToTrain);

        NLPUtils.saveModelBackup("/" + docTrain.getLanguage() + modelName, modelFilePath, modelBckpFilePath, timeInMills);

        trainDocumentTask(docTrain, "ner", modelFilePath + "/" + docTrain.getLanguage() + modelName, trainDataFile);

        saveModelTrainingInfo("/" + docTrain.getLanguage() + modelName, trainDataFile, timeInMills, lines);
    }

    /**
     * In case of the NER model training it can train three different models by entity.
     * Based on the {@code entityToTrain} it returns the path to the desired model
     *
     * @param entityToTrain defines which model of NER it trains [person, location, organization]
     * @return NER model filepath to train
     */
    private String getCorrectModel(String entityToTrain)
    {
        switch (entityToTrain)
        {
            case "person":
                return Constants.nerPersonModelFile;
            case "location":
                return Constants.nerLocationModelFile;
            case "organization":
                return Constants.nerOrganizationModelFile;
            default:
                return "";
        }
    }

    /**
     * In case of the NER model training it can train on thre differend corrected corpuses.
     * Based on the {@code entityToTrain} it returns the path to the desired training file.
     *
     * @param entityToTrain defines which model of NER it trains [person, location, organization]
     * @return NER model filepath to train
     */
    private String getCorrectTrain(String entityToTrain)
    {
        switch (entityToTrain)
        {
            case "person":
                return Constants.correctedCorpus;
            case "location":
                return Constants.correctedLocCorpus;
            case "organization":
                return Constants.correctedOrgCorpus;
            default:
                return "";
        }
    }

    /**
     * When the training is successful it saves the training info into the MongoDB database
     *
     * @param modelFile model that was trained
     * @param trainDataFile file containing the training data that was used
     * @param versionName version of the training that has the value of timestamp in milliseconds
     * @param lines contains the lines that were added to the old training corpus (database)
     */
    private void saveModelTrainingInfo(String modelFile, String trainDataFile, Long versionName, List<String> lines)
    {

        modelTrainInfo.setModelName((modelTrainInfo.getModelName() != null ? modelTrainInfo.getVersionName() + ", " + versionName : versionName.toString()));

        logger.info("Saving model training info to MongoDB...");
        ModelTrainInfo modelTrain = new ModelTrainInfo();
        modelTrain.setModelName(modelFile);
        modelTrain.setTimeStamp(Calendar.getInstance().getTime());
        modelTrain.setTrainDataFile(trainDataFile);
        modelTrain.setVersionName(versionName);
        modelTrain.setAdditionalTrainData(lines);

        modelTrainRepository.save(modelTrain);
        logger.info("Saved training info to MongoDB!");
    }

    /**
     * Find all the training models that were trained in the MongoDB.
     *
     * @return list of old training sessions
     */
    public List<ModelTrainInfo> findAllModels()
    {
        logger.info("Retriving model training info from MongoDB...");
        return modelTrainRepository.findAll();
    }

    /**
     * Method retrains the model with corrected corpus from the client side, which is saved in the
     * ./data/OpenNLP/upload folder.
     *
     * @param modelType which has to be trained
     */
    public void retrainNerModel(String modelType)
    {
        logger.info("Starting to train new model with corrected corpus.");

        String modelFile = getCorrectModel(modelType);

        List<String> sentenceDiffs = sentenceDiffRepository.findAllByModelOrderByCorpusLine(modelType)
                .stream()
                .map(CorpusSentenceDiff::getSentence)
                .collect(Collectors.toList());

        if (sentenceDiffs.size() < 1)
        {
            logger.warn("There were no lines in the database to retrain on");
        }
        else
        {
            try
            {
                FileUtils.writeLines(new File(nerTrainDir + getCorrectTrain(modelType)), sentenceDiffs);
            }
            catch (IOException ex)
            {
                logger.warn("There was an error when saving sentences from MongoDB to files.");
            }
        }

        logger.info("Starting to train model...");
        String correctedCorpus = uploadDir + getCorrectTrain(modelType);
        NERTask task = new NERTask();
        try
        {
            logger.info("Training new model...");
            task.train(modelFile, correctedCorpus, "slo");
            logger.info("Re-train was successful!");
        }
        catch (IOException ex)
        {
            logger.warn("There was an error when processing model re-train.");
        }

        sentenceDiffRepository.removeByModel(modelType);
        logger.info("Starting tagging of model training data...");
        startTaggingDictAndStat(modelType);
    }

    /**
     *
     */
    public void startTaggingDictAndStat(String modelType)
    {
        logger.info("Starting to tag untagged training data...");
        NERTask task = new NERTask();
        List<CorpusSentenceDiff> diffList = task.executeNewModelTagging(dataDir, dictionaryDir, modelType, modelFilePath);
        sentenceDiffRepository.saveAll(diffList);
        logger.info("Tagging of corpus data has been successful!");
    }

    public List<CorpusSentenceDiff> modelSentencesBetween(String modelType, Integer start, Integer stop)
    {
        logger.info("Searching for sentences in model " + modelType + " for lines between " + start + " and " + stop + "...");
        return sentenceDiffRepository.findByModelAndCorpusLineBetween(modelType, start, stop);
    }

    public void saveCorrectionOfCorpus(List<CorpusSentenceDiff> sentenceDiffList)
    {
        logger.info("Saving corrected sentences to the corpus...");
        sentenceDiffRepository.saveAll(sentenceDiffList);
        logger.info("Correction has been saved to MongoDB!");
    }

    public CorpusSentenceDiff saveCorpusSentenceCorrection(CorpusSentenceDiff sentenceDiff)
    {
        logger.info("Saving corrected sentence to the corpus...");
        logger.info("Sentence is at line: " + sentenceDiff.getCorpusLine() + " for model: " + sentenceDiff.getModel());
        return sentenceDiffRepository.save(sentenceDiff);
    }

    public CorpusSentenceDiff findModelsFirstLine(String modelType)
    {
        logger.info("Searching entry in sentence diff for model " + modelType + "...");
        return sentenceDiffRepository.findByModelAndCorpusLine(modelType, 1);
    }
}
